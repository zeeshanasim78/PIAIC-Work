{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0Y3P6KBOfr9v"
   },
   "outputs": [],
   "source": [
    "# Session 4 : 09Jul23\n",
    "# by Sir Nasir\n",
    "# Chapter # 6 :  Deep learning for text and sequences\n",
    "# Topic  : 6.1 :  Working with text data\n",
    "\n",
    "# Text is one of the most widespread forms of sequence data. It can be understood as either a sequence of characters or a sequence of words, but it’s most common to\n",
    "# work at the level of words.\n",
    "\n",
    "# Like all other neural networks, deep-learning models don’t take as input raw text: they only work with numeric tensors.\n",
    "# Vectorizing text is the process of transforming text into numeric tensors. This can be done in multiple ways:\n",
    "# a) Segment text into words, and transform each word into a vector.\n",
    "# b) Segment text into characters, and transform each character into a vector.\n",
    "# c) Extract n-grams of words or characters, and transform each n-gram into a vector. (N-grams are overlapping groups of multiple consecutive words or characters.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FWWPjBUPzyVP"
   },
   "outputs": [],
   "source": [
    "# Understanding n-grams and bag-of-words\n",
    "\n",
    "# Collectively, the different units into which you can break down text (words, characters, or n-grams) are called tokens, and breaking text into such tokens is called\n",
    "# tokenization. All text-vectorization processes consist of applying some tokenization scheme and then associating numeric vectors with the generated tokens.\n",
    "\n",
    "# There are multiple ways to associate a vector with a token. In this section, I’ll present two major ones: one-hot encoding of tokens, and token embedding which is\n",
    "# typically used exclusively for words, and called word embedding.\n",
    "\n",
    "# Word n-grams are groups of N (or fewer) consecutive words that you can extract from a sentence. The same concept may also be applied to characters instead of words.\n",
    "# Here’s a simple example. Consider the sentence “The cat sat on the mat.” It may be decomposed into the following set of 2-grams:\n",
    "#     {\"The\", \"The cat\", \"cat\", \"cat sat\", \"sat\", \"sat on\", \"on\", \"on the\", \"the\", \"the mat\", \"mat\"}\n",
    "# It may also be decomposed into the following set of 3-grams:\n",
    "#     {\"The\", \"The cat\", \"cat\", \"cat sat\", \"The cat sat\", \"sat\", \"sat on\", \"on\", \"cat sat on\", \"on the\", \"the\", \"sat on the\", \"the mat\", \"mat\", \"on the mat\"}\n",
    "# Such a set is called a bag-of-2-grams or bag-of-3-grams, respectively. The term bag here refers to the fact that you’re dealing with a set of tokens rather than a\n",
    "#  list or sequence: the tokens have no specific order. This family of tokenization methods is called bag-of-words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EovnthFe04qW",
    "outputId": "bc6c6b06-3f9b-41a7-ebeb-ed726294b737"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'The': 1, 'cat': 2, 'sat': 3, 'on': 4, 'the': 5, 'mat.': 6, 'dog': 7, 'ate': 8, 'my': 9, 'homework.': 10} \n",
      "\n",
      "[[[0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "\n",
      " [[0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]]\n"
     ]
    }
   ],
   "source": [
    "# 6.1.1 One-hot encoding of words and characters\n",
    "# One-hot encoding is the most common, most basic way to turn a token into a vector. It consists of associating a unique integer index with every word\n",
    "# and then turning this integer index i into a binary vector of size N (the size of the vocabulary); the vector is all zeros except for the ith entry, which is 1.\n",
    "\n",
    "# Listing 6.1 Word-level one-hot encoding (toy example)\n",
    "import numpy as np\n",
    "\n",
    "# Initial data: one entry per sample (in this example, a sample is a sentence, but it could be an entire document)\n",
    "samples = ['The cat sat on the mat.', 'The dog ate my homework.']\n",
    "\n",
    "# Builds an index of all tokens in the data\n",
    "token_index = {}\n",
    "for sample in samples:\n",
    "  # Tokenizes the samples via the split method. In real life, you’d also strip punctuation and special characters from the samples.\n",
    "  for word in sample.split():\n",
    "    if word not in token_index:\n",
    "      token_index[word] = len(token_index) + 1  # Assigns a unique index to each unique word. Note that you don’t attribute index 0 to anything.\n",
    "\n",
    "print(token_index,\"\\n\")\n",
    "\n",
    "max_length = 10  # Vectorizes the samples. You’ll only consider the first max_length words in each sample.\n",
    "\n",
    "\n",
    "results = np.zeros(shape=(len(samples), max_length,\n",
    "                          max(token_index.values()) + 1))  # This is where you store the results.\n",
    "\n",
    "for i, sample in enumerate(samples):\n",
    "  for j, word in list(enumerate(sample.split()))[:max_length]:\n",
    "    index = token_index.get(word)\n",
    "    results[i, j, index] = 1\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A85jWMz3-VSf",
    "outputId": "e07c106e-2006-468e-ac83-9d2f9d4b20ee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: '0', 2: '1', 3: '2', 4: '3', 5: '4', 6: '5', 7: '6', 8: '7', 9: '8', 10: '9', 11: 'a', 12: 'b', 13: 'c', 14: 'd', 15: 'e', 16: 'f', 17: 'g', 18: 'h', 19: 'i', 20: 'j', 21: 'k', 22: 'l', 23: 'm', 24: 'n', 25: 'o', 26: 'p', 27: 'q', 28: 'r', 29: 's', 30: 't', 31: 'u', 32: 'v', 33: 'w', 34: 'x', 35: 'y', 36: 'z', 37: 'A', 38: 'B', 39: 'C', 40: 'D', 41: 'E', 42: 'F', 43: 'G', 44: 'H', 45: 'I', 46: 'J', 47: 'K', 48: 'L', 49: 'M', 50: 'N', 51: 'O', 52: 'P', 53: 'Q', 54: 'R', 55: 'S', 56: 'T', 57: 'U', 58: 'V', 59: 'W', 60: 'X', 61: 'Y', 62: 'Z', 63: '!', 64: '\"', 65: '#', 66: '$', 67: '%', 68: '&', 69: \"'\", 70: '(', 71: ')', 72: '*', 73: '+', 74: ',', 75: '-', 76: '.', 77: '/', 78: ':', 79: ';', 80: '<', 81: '=', 82: '>', 83: '?', 84: '@', 85: '[', 86: '\\\\', 87: ']', 88: '^', 89: '_', 90: '`', 91: '{', 92: '|', 93: '}', 94: '~', 95: ' ', 96: '\\t', 97: '\\n', 98: '\\r', 99: '\\x0b', 100: '\\x0c'} \n",
      "\n",
      "[[[1. 1. 1. ... 1. 1. 1.]\n",
      "  [1. 1. 1. ... 1. 1. 1.]\n",
      "  [1. 1. 1. ... 1. 1. 1.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      " [[1. 1. 1. ... 1. 1. 1.]\n",
      "  [1. 1. 1. ... 1. 1. 1.]\n",
      "  [1. 1. 1. ... 1. 1. 1.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]]\n"
     ]
    }
   ],
   "source": [
    "# Listing 6.2 Character-level one-hot encoding (toy example)\n",
    "import string\n",
    "\n",
    "samples = ['The cat sat on the mat.', 'The dog ate my homework.']\n",
    "characters = string.printable # All printable ASCII characters\n",
    "\n",
    "token_index = dict(zip(range(1, len(characters) + 1), characters))\n",
    "print(token_index,\"\\n\")\n",
    "\n",
    "max_length = 50\n",
    "\n",
    "results = np.zeros((len(samples), max_length, max(token_index.keys()) + 1))\n",
    "\n",
    "for i, sample in enumerate(samples):\n",
    "  for j, character in enumerate(sample):\n",
    "    index = token_index.get(character)\n",
    "    results[i, j, index] = 1\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9g67dhchCWT_",
    "outputId": "6a29a38c-2c0f-45a1-e7f8-fef5c69fcdd4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 9 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "# Listing 6.3 Using Keras for word-level one-hot encoding\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "samples = ['The cat sat on the mat.', 'The dog ate my homework.']\n",
    "\n",
    "# Creates a tokenizer, configured to only take into account the 1,000 most common words\n",
    "tokenizer = Tokenizer(num_words=1000)\n",
    "\n",
    "# Builds the word index\n",
    "tokenizer.fit_on_texts(samples)\n",
    "\n",
    "# Turns strings into lists of integer indices\n",
    "sequences = tokenizer.texts_to_sequences(samples)\n",
    "\n",
    "# You could also directly get the one-hot binary representations. Vectorization modes other than one-hot encoding are supported by this tokenizer.\n",
    "one_hot_results = tokenizer.texts_to_matrix(samples, mode='binary')\n",
    "\n",
    "# How you can recover the word index that was computed\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listing 6.4 Word-level one-hot encoding with hashing trick (toy example)\n",
    "\n",
    "# A variant of one-hot encoding is the so-called one-hot hashing trick, which you can use when the number of unique tokens in your vocabulary is too large to handle\n",
    "# explicitly. Instead of explicitly assigning an index to each word and keeping a reference of these indices in a dictionary, you can hash words into vectors of fixed\n",
    "# size. This is typically done with a very lightweight hashing function.\n",
    "\n",
    "# The main advantage of this method is that it does away with maintaining an explicit word index, which saves memory and allows online encoding of the data\n",
    "# The one drawback of this approach is that it’s susceptible to hash collisions: two different words may end up with the same hash, and subsequently any\n",
    "# machine-learning model looking at these hashes won’t be able to tell the difference between these words.\n",
    "\n",
    "samples = ['The cat sat on the mat.', 'The dog ate my homework.']\n",
    "\n",
    "# Stores the words as vectors of size 1,000. If you have close to 1,000 words (or more), you’ll see many hash collisions, which will decrease the accuracy of this encoding method.\n",
    "dimensionality = 1000\n",
    "\n",
    "max_length = 10\n",
    "\n",
    "results = np.zeros((len(samples), max_length, dimensionality))\n",
    "\n",
    "for i, sample in enumerate(samples):\n",
    "  for j, word in list(enumerate(sample.split()))[:max_length]:\n",
    "    index = abs(hash(word)) % dimensionality  # Hashes the word into a random integer index between 0 and 1,000\n",
    "    results[i, j, index] = 1\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
