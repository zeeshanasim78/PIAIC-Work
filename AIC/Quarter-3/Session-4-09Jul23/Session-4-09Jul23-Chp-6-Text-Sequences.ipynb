{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d50a223",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deep learning for text and sequence\n",
    "# Chapter 6 of Deep Learning with Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa56a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.1 Working with text data : \n",
    "# Text is one of the most widespread forms of sequence data. It can be understood as either a sequence of characters or a\n",
    "# sequence of words, but it’s most common to work at the level of words. The deep-learning sequence-processing models \n",
    "# introduced in the following sections can use text to produce a basic form of natural-language understanding, sufficient for\n",
    "# applications including document classification, sentiment analysis, author identification, and even question-answering (QA) \n",
    "# (in a constrained context). Of course, keep in mind throughout this chapter that none of these deeplearning models truly \n",
    "# understand text in a human sense; rather, these models can map the statistical structure of written language, which is \n",
    "# sufficient to solve many simple textual tasks. Deep learning for natural-language processing is pattern recognition\n",
    "# applied to words, sentences, and paragraphs, in much the same way that computer vision is pattern recognition applied to pixels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5dec2e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Like all other neural networks, deep-learning models don’t take as input raw text. They only work with numeric tensors.\n",
    "# Vectorizing text is the process of transforming text into numeric tensors. This can be done in multiple ways:\n",
    "# ** Segment text into words, and transform each word into a vector.\n",
    "# ** Segment text into characters, and transform each character into a vector.\n",
    "# ** Extract n-grams of words or characters, and transform each n-gram into a vector. N-grams are overlapping groups of multiple consecutive words or characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d30de67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collectively, the different units into which you can break down text (words, characters, or n-grams) are called tokens, \n",
    "# and breaking text into such tokens is called tokenization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99133f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.1.1 One-hot encoding of words and characters\n",
    "# One-hot encoding is the most common, most basic way to turn a token into a vector. You saw it in action in the initial IMDB \n",
    "# and Reuters examples in chapter 3 (done with words, in that case). It consists of associating a unique integer index with\n",
    "# every word and then turning this integer index i into a binary vector of size N (the size of the vocabulary); the vector is \n",
    "# all zeros except for the ith entry, which is 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0fbd8624",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'The': 1, 'cat': 2, 'sat': 3, 'on': 4, 'the': 5, 'mat.': 6, 'dog': 7, 'ate': 8, 'my': 9, 'homework.': 10}\n",
      "[[[0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "\n",
      " [[0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]]\n"
     ]
    }
   ],
   "source": [
    "# Listing 6.1 Word-level one-hot encoding (toy example)\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Initial data: one entry per sample (in this example, a sample is a sentence, but it could be an entire document)\n",
    "samples = ['The cat sat on the mat.', 'The dog ate my homework.']\n",
    "\n",
    "# Builds an index of all tokens in the data\n",
    "token_index = {}\n",
    "for sample in samples:\n",
    "    for word in sample.split(): # # Tokenizes the samples via the split method. In real life, you’d also strip punctuation and special characters from the samples.\n",
    "        if word not in token_index:\n",
    "            token_index[word] = len(token_index) + 1  # ssigns a unique index to each unique word. Note that you don’t attribute index 0 to anything.\n",
    "\n",
    "# Just to view the Token Index\n",
    "print(token_index)\n",
    "\n",
    "max_length = 10 # Vectorizes the samples. You’ll only consider the first max_length words in each sample.\n",
    "\n",
    "# This is where we will store the results.\n",
    "results = np.zeros(shape=(len(samples), max_length, max(token_index.values()) + 1)) \n",
    "\n",
    "\n",
    "\n",
    "for i, sample in enumerate(samples):\n",
    "    for j, word in list(enumerate(sample.split()))[:max_length]:\n",
    "        index = token_index.get(word)\n",
    "        results[i, j, index] = 1\n",
    "\n",
    "print(results)        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50a13592",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
